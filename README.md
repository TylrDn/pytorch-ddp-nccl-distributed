# pytorch-ddp-nccl-distributed
Turn-key PyTorch DistributedDataParallel (DDP) with NCCL for single- and multi-node GPU training. Includes Docker image, K8s Job and optional Kubeflow PyTorchJob, Slurm sbatch, sensible NCCL env defaults, Makefile, and CI. Great for demonstrating distributed training fundamentals.
